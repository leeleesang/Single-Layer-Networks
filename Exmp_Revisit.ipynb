{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "second-philippines",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run SetUp.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-bhutan",
   "metadata": {},
   "source": [
    "Configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "upset-detector",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues of H = tensor([17.1551,  5.1156,  0.7293])\n",
      "w* = tensor([ 1.0000e-02,  2.4950e+00, -1.1455e-07])\n"
     ]
    }
   ],
   "source": [
    "## Configuration\n",
    "Title = \"Exmp_Revisit\"\n",
    "device = 'cpu' # \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "output_path = f\"./Results/{Title}\"\n",
    "create_directory(output_path)\n",
    "\n",
    "\n",
    "# data setting\n",
    "n=3 # number of data\n",
    "d=3 # dimension of data\n",
    "\n",
    "\n",
    "# input X is shape of n x d \n",
    "X = torch.tensor([[1, 0, 4], # x1^T\n",
    "                  [1, 2, 0], # x2^T\n",
    "                  [1, 0., 0]], # x3^T\n",
    "                 device=device).float()\n",
    "# label Y is shape of n x 1\n",
    "Y = torch.tensor([[0.01], [5], [0.01]], device=device).float()\n",
    "\n",
    "H = X.t().matmul(X) # The Hessian Matirx\n",
    "q = (X * Y).sum(0)\n",
    "\n",
    "U, D, _ = torch.svd(H) # eigenvector decompsition of H\n",
    "print(\"eigenvalues of H =\", D)\n",
    "w_star = torch.linalg.inv(H).matmul(q) # the global minimum of the linear network\n",
    "w_star_cpu = w_star.cpu()\n",
    "zero = np.array([0,0,0.])\n",
    "print(\"w* =\", w_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "south-cycle",
   "metadata": {},
   "source": [
    "Single-Layer Linear and ReLU Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "julian-engineering",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w0= tensor([[2.7140e-05, 4.4114e-06, 8.7711e-05]])\n"
     ]
    }
   ],
   "source": [
    "# Single-Layer Linear and ReLU Networks\n",
    "\n",
    "class Linear_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Linear_Net, self).__init__()\n",
    "        self.module = nn.Sequential(\n",
    "            nn.Linear(d, 1, bias=False)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.module(x) \n",
    "    \n",
    "    def weight(self):\n",
    "        return self.module[0].weight.detach().cpu()\n",
    "\n",
    "\n",
    "class ReLU_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ReLU_Net, self).__init__()\n",
    "        self.module = nn.Sequential(\n",
    "            nn.Linear(d, 1, bias=False),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.module(x) \n",
    "    \n",
    "    def weight(self):\n",
    "        return self.module[0].weight.detach().cpu()\n",
    "\n",
    "\n",
    "w0 = 0.0001 * torch.rand(1,d) # initialization with small norm\n",
    "print(\"w0=\", w0)\n",
    "\n",
    "# \n",
    "linear_net = Linear_Net()\n",
    "linear_net.module[0].weight = nn.Parameter(copy.deepcopy(w0)) \n",
    "linear_net.to(device)\n",
    "relu_net = ReLU_Net()\n",
    "relu_net.module[0].weight = nn.Parameter(copy.deepcopy(w0)) \n",
    "relu_net.to(device)\n",
    "\n",
    "\n",
    "# optimization Setting\n",
    "lr = 0.005\n",
    "criterion = nn.MSELoss()\n",
    "linear_optimizer = optim.SGD(linear_net.parameters(), lr=lr)\n",
    "relu_optimizer = optim.SGD(relu_net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "choice-shark",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "linear_tr = np.empty((0,3)) # Gradient flow of the linear network\n",
    "relu_tr = np.empty((0,3)) # Gradient flow of the ReLU network\n",
    "linear_loss_tr = np.empty(0) # Loss of the ReLU network\n",
    "relu_loss_tr = np.empty(0) # Loss of the ReLU network\n",
    "\n",
    "Epochs = 10000\n",
    "log_period = 10\n",
    "for epoch in range(Epochs) :\n",
    "    linear_loss = criterion(linear_net(X), Y)\n",
    "    linear_loss.backward()\n",
    "    linear_optimizer.step()\n",
    "    linear_net.zero_grad()\n",
    "    \n",
    "    relu_loss = criterion(relu_net(X), Y)\n",
    "    relu_loss.backward()\n",
    "    relu_optimizer.step()\n",
    "    relu_net.zero_grad()\n",
    "    \n",
    "#     if (net(X)>0).prod().item() != 1 :\n",
    "#         print(f\"At epoch {epoch}, deactivation occurs !!!!!!!!!!\")\n",
    "#         print(f\"output is {net(X).detach()}\")    \n",
    "    \n",
    "    if epoch % log_period == 0 :\n",
    "        linear_tr = np.append(linear_tr, linear_net.weight(), axis=0)\n",
    "        relu_tr = np.append(relu_tr, relu_net.weight(), axis=0)\n",
    "        linear_loss_tr = np.append(linear_loss_tr, linear_loss.item())\n",
    "        relu_loss_tr = np.append(relu_loss_tr, relu_loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
